{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc73d08d-19f6-49aa-a85a-2d2d52c581da",
   "metadata": {},
   "source": [
    "# Import pose estimation model\n",
    "\n",
    "## Define output format\n",
    "\n",
    "Let's load the JSON file which describes the human pose task.  This is in COCO format, it is the category descriptor pulled from the annotations file.  We modify the COCO category slightly, to add a neck keypoint.  We will use this task description JSON to create a topology tensor, which is an intermediate data structure that describes the part linkages, as well as which channels in the part affinity field each linkage corresponds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a34bead-4989-4e7b-8c7f-2f09b611375f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + \"/configs/\" # Specify MatplotLib config folder\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "# Requiere https://github.com/NVIDIA-AI-IOT/trt_pose\n",
    "import trt_pose.coco\n",
    "from trt_pose.draw_objects import DrawObjects\n",
    "from trt_pose.parse_objects import ParseObjects\n",
    "\n",
    "with open('human_pose.json', 'r') as f:\n",
    "    human_pose = json.load(f)\n",
    "\n",
    "topology = trt_pose.coco.coco_category_to_topology(human_pose)\n",
    "\n",
    "parse_objects = ParseObjects(topology)\n",
    "draw_objects = DrawObjects(topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c31468-df1c-43c2-bd59-40c50401e2e0",
   "metadata": {},
   "source": [
    "## Import TensorRT optimized model\n",
    "\n",
    "Next, we'll load our model. It has been optimized using another Notebook and saved so that we do not need to perform optimization again, we can just load the model. Please note that TensorRT has device specific optimizations, so you can only use an optimized model on similar platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc10e8eb-786c-438e-810f-77f069f5ff0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Requiere https://github.com/NVIDIA-AI-IOT/torch2trt\n",
    "from torch2trt import TRTModule\n",
    "\n",
    "OPTIMIZED_MODEL = 'resnet18_baseline_att_224x224_A_epoch_249_trt.pth'\n",
    "\n",
    "model_trt = TRTModule()\n",
    "model_trt.load_state_dict(torch.load(OPTIMIZED_MODEL))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbed1cb-6960-4223-bb9e-fe2671f69e8d",
   "metadata": {},
   "source": [
    "# Define video-processing pipeline\n",
    "\n",
    "## Pre-process image for TRT_Pose\n",
    "\n",
    "Next, let's define a function that will preprocess the image, which is originally in BGR8 / HWC format. It is formated to the default Torch format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b4edef-b585-4a5e-a6a7-c150e3a026ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision.transforms as transforms\n",
    "import PIL.Image\n",
    "\n",
    "mean = torch.Tensor([0.485, 0.456, 0.406]).cuda()\n",
    "std = torch.Tensor([0.229, 0.224, 0.225]).cuda()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "def preprocess(image):\n",
    "    global device\n",
    "    device = torch.device('cuda')\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device)\n",
    "    image.sub_(mean[:, None, None]).div_(std[:, None, None])\n",
    "    return image[None, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60b9d20-30cb-4f17-a528-e87b1678a64e",
   "metadata": {},
   "source": [
    "## Access video feed\n",
    "\n",
    "Access images streamed by a WiFi camera on the local network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79780417-b089-420f-9a50-8473a177a4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50090d3b306946ef9743b8ac4b26779c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "WIDTH_INPUT, HEIGHT_INPUT = 224, 224 # Imposed by the model\n",
    "\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "import urllib.request\n",
    "\n",
    "image_w = ipywidgets.Image(format='jpeg')\n",
    "\n",
    "display(image_w)\n",
    "\n",
    "url_esp32 = 'http://192.168.0.163/capture'\n",
    "url_IPcam = 'http://192.168.0.244:8080/photo.jpg'\n",
    "def fetch_image(url):\n",
    "    imgResp = urllib.request.urlopen(url)\n",
    "    imgNp = np.array(bytearray(imgResp.read()),dtype=np.uint8)\n",
    "    img = cv2.imdecode(imgNp,-1)\n",
    "    img_height, img_width, img_channel = img.shape\n",
    "    if img_width>img_height:\n",
    "        img = cv2.resize(img, (int((HEIGHT_INPUT/img_height)*img_width), HEIGHT_INPUT), interpolation = cv2.INTER_AREA)\n",
    "        img = img[:, img.shape[1]//2 - WIDTH_INPUT//2 : img.shape[1]//2 + WIDTH_INPUT//2]\n",
    "    else:\n",
    "        img = cv2.resize(img, (WIDTH_INPUT, int((WIDTH_INPUT/img_width)*img_height)), interpolation = cv2.INTER_AREA)\n",
    "        img = img[img.shape[0] - HEIGHT_INPUT//2 : img.shape[0] + HEIGHT_INPUT//2,:]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668cdbd-3cf0-4b83-9549-bf93ba2fd8d8",
   "metadata": {},
   "source": [
    "## Get keypoints with TRT-Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481c71a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(counts, objects, peak, indexBody=0):\n",
    "    #if indexBody<counts[0]:\n",
    "    #    return None\n",
    "    kpoint = []\n",
    "    human = objects[0][indexBody]\n",
    "    C = human.shape[0]\n",
    "    for j in range(C):\n",
    "        k = int(human[j])\n",
    "        if k >= 0:\n",
    "            peak = peaks[0][j][k]   # peak[1]:width, peak[0]:height\n",
    "            kpoint.append([float(peak[1]),float(peak[0])])\n",
    "            #print('indexBody:%d : success [%5.3f, %5.3f]'%(j, peak[1], peak[2]) )\n",
    "        else:\n",
    "        \n",
    "            kpoint.append([None, None])\n",
    "            #print('indexBody:%d : None'%(j) )\n",
    "    return np.array(kpoint)\n",
    "\n",
    "def get_cmap_paf(image):\n",
    "        data = preprocess(image)\n",
    "        cmap, paf = model_trt(data)\n",
    "        cmap, paf = cmap.detach().cpu(), paf.detach().cpu()\n",
    "        return cmap, paf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97864604-2798-4243-941c-47a3e52620ea",
   "metadata": {},
   "source": [
    "## Get label with pose-classification-kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20bbb66-1951-4693-bf79-f9f68da81c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CNN_BODY18_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 16, 16)            112       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16, 16)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 14, 32)            1568      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 14, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 12, 32)            3104      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               49280     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                660       \n",
      "=================================================================\n",
      "Total params: 65,060\n",
      "Trainable params: 65,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "labels: ['Seated', 'Stand', 'Stand_RightArmRaised', 'Stand_LeftArmRaised', 'T', 'MilitarySalute', 'PushUp_Low', 'Squat', 'Plank', 'Yoga_Tree_left', 'Yoga_Tree_right', 'Yoga_UpwardSalute', 'Yoga_Warrior2_left', 'Yoga_Warrior2_right', 'Traffic_AllStop', 'Traffic_BackStop', 'Traffic_FrontStop', 'Traffic_BackFrontStop', 'Traffic_LeftTurn', 'Traffic_RightTurn']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "classificationModelURL = \"https://github.com/ArthurFDLR/pose-classification-kit/blob/master/pose_classification_kit/models/Body/CNN_BODY18_1/CNN_BODY18_1.h5?raw=true\"\n",
    "classificationModelPath = get_file(\n",
    "    \"CNN_BODY18_1\",\n",
    "    classificationModelURL\n",
    ")\n",
    "classificationModel = keras.models.load_model(classificationModelPath)\n",
    "classificationModel.summary()\n",
    "\n",
    "classificationLabelsURL = \"https://raw.githubusercontent.com/ArthurFDLR/pose-classification-kit/master/pose_classification_kit/models/Body/CNN_BODY18_1/class.json\"\n",
    "classificationLabelsPath = get_file(\n",
    "    \"CNN_BODY18_1_Info\",\n",
    "    classificationLabelsURL\n",
    ")\n",
    "\n",
    "with open(classificationLabelsPath) as f:\n",
    "    classificationLabels = json.load(f)['labels']\n",
    "print(\"labels:\", classificationLabels)\n",
    "\n",
    "def getLengthLimb(data, keypoint1: int, keypoint2: int):\n",
    "    if type(data[keypoint1, 0]) != type(None) and type(data[keypoint2, 0]) != type(None):\n",
    "        return np.linalg.norm([data[keypoint1, 0:2] - data[keypoint2, 0:2]])\n",
    "    return 0\n",
    "\n",
    "def preprocess_keypoints(keypoints:np.ndarray):\n",
    "    if type(keypoints) != type(None):\n",
    "        assert keypoints.shape == (18,2)\n",
    "        # Find bounding box\n",
    "        min_x, max_x = float(\"inf\"), 0.0\n",
    "        min_y, max_y = float(\"inf\"), 0.0\n",
    "        for k in keypoints:\n",
    "            if type(k[0]) != type(None):  # If keypoint exists\n",
    "                min_x = min(min_x, k[0])\n",
    "                max_x = max(max_x, k[0])\n",
    "                min_y = min(min_y, k[1])\n",
    "                max_y = max(max_y, k[1])\n",
    "\n",
    "        # Centering\n",
    "        np.subtract(\n",
    "            keypoints[:, 0],\n",
    "            (min_x + max_x) / 2.,\n",
    "            where=keypoints[:, 0] != None,\n",
    "            out=keypoints[:, 0],\n",
    "        )\n",
    "        np.subtract(\n",
    "            (min_y + max_y) / 2.,\n",
    "            keypoints[:, 1],\n",
    "            where=keypoints[:, 0] != None,\n",
    "            out=keypoints[:, 1],\n",
    "        )\n",
    "\n",
    "        # Scaling  \n",
    "        normalizedPartsLength = np.array(\n",
    "            [\n",
    "                getLengthLimb(keypoints, 6, 12) * (16.0 / 5.2),  # Torso right\n",
    "                getLengthLimb(keypoints, 5, 11) * (16.0 / 5.2),  # Torso left\n",
    "                getLengthLimb(keypoints, 0, 17) * (16.0 / 2.5),  # Neck\n",
    "                getLengthLimb(keypoints, 12, 14) * (16.0 / 3.6),  # Right thigh\n",
    "                getLengthLimb(keypoints, 14, 16) * (16.0 / 3.5),  # Right lower leg\n",
    "                getLengthLimb(keypoints, 11, 13) * (16.0 / 3.6),  # Left thigh\n",
    "                getLengthLimb(keypoints, 13, 15) * (16.0 / 3.5),  # Left lower leg\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Mean of non-zero lengths\n",
    "        normalizedPartsLength = normalizedPartsLength[normalizedPartsLength > 0.0]\n",
    "        if len(normalizedPartsLength)>0:\n",
    "            scaleFactor = np.mean(normalizedPartsLength)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        # Populate None keypoints with 0s\n",
    "        keypoints[keypoints == None] = 0.0\n",
    "\n",
    "        # Normalize\n",
    "        np.divide(keypoints, scaleFactor, out=keypoints[:, 0:2])\n",
    "\n",
    "        if np.any((keypoints > 1.0) | (keypoints < -1.0)):\n",
    "            #print(\"Scaling error\")\n",
    "            return None\n",
    "\n",
    "        return keypoints.astype('float32')\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d5c2ad-6768-403a-bf73-0c09982af1bf",
   "metadata": {},
   "source": [
    "# Main Processing loop\n",
    "\n",
    "- Read image\n",
    "- Infere keypoints\n",
    "- Infere label\n",
    "- Send MQTT update\n",
    "- Draw skeleton on the input image\n",
    "- Update in output window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7445e59f-e997-423a-88a8-0c650c5685c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing stopped\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        # Get image\n",
    "        image = fetch_image(url_esp32)\n",
    "        \n",
    "        # TRT-Pose inference\n",
    "        cmap, paf = get_cmap_paf(image)\n",
    "        counts, objects, peaks = parse_objects(cmap, paf)\n",
    "        keypoints = get_keypoints(counts, objects, peaks)\n",
    "        \n",
    "        # Classification inference\n",
    "        label_pose = None\n",
    "        keypoints = preprocess_keypoints(keypoints)\n",
    "        if type(keypoints) != type(None):\n",
    "            prediction = classificationModel.predict(x=np.array([keypoints]))\n",
    "            label_pose = classificationLabels[np.argmax(prediction)]\n",
    "\n",
    "        # Display image locally\n",
    "        draw_objects(image, counts, objects, peaks)\n",
    "        if label_pose:\n",
    "            image = cv2.putText(image, label_pose, (10,20), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                   .7, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "        image_w.value = bytes(cv2.imencode('.jpg', image[:, :, :])[1])\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print('Video processing stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce805dcb-2f37-4405-81d1-2b1bd75f79b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
